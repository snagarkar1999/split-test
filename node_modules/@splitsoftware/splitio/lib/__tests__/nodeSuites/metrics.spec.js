"use strict";

var _interopRequireDefault = require("@babel/runtime-corejs3/helpers/interopRequireDefault");

var _Object$defineProperty = require("@babel/runtime-corejs3/core-js-stable/object/define-property");

require("core-js/modules/es.function.name");

require("core-js/modules/es.regexp.constructor");

require("core-js/modules/es.regexp.exec");

require("core-js/modules/es.regexp.to-string");

_Object$defineProperty(exports, "__esModule", {
  value: true
});

exports.default = _default;

var _reduce = _interopRequireDefault(require("@babel/runtime-corejs3/core-js-stable/instance/reduce"));

var _isArray = _interopRequireDefault(require("@babel/runtime-corejs3/core-js-stable/array/is-array"));

var _find = _interopRequireDefault(require("@babel/runtime-corejs3/core-js-stable/instance/find"));

var _some = _interopRequireDefault(require("@babel/runtime-corejs3/core-js-stable/instance/some"));

var _filter = _interopRequireDefault(require("@babel/runtime-corejs3/core-js-stable/instance/filter"));

var _regenerator = _interopRequireDefault(require("@babel/runtime-corejs3/regenerator"));

var _now = _interopRequireDefault(require("@babel/runtime-corejs3/core-js-stable/date/now"));

require("regenerator-runtime/runtime");

var _asyncToGenerator2 = _interopRequireDefault(require("@babel/runtime-corejs3/helpers/asyncToGenerator"));

var _ = require("../../");

var _settings = _interopRequireDefault(require("../../utils/settings"));

var _splitchangesSince = _interopRequireDefault(require("../mocks/splitchanges.since.-1.json"));

var _splitchangesSince2 = _interopRequireDefault(require("../mocks/splitchanges.since.1457552620999.json"));

var baseUrls = {
  sdk: 'https://sdk.baseurl/metricsSuite',
  events: 'https://events.baseurl/metricsSuite'
};
var settings = (0, _settings.default)({
  core: {
    key: '<fake id>'
  },
  urls: baseUrls
});
var config = {
  core: {
    authorizationKey: '<fake-token-2>'
  },
  scheduler: {
    featuresRefreshRate: 99999,
    segmentsRefreshRate: 99999,
    metricsRefreshRate: 3,
    impressionsRefreshRate: 99999
  },
  urls: baseUrls,
  startup: {
    eventsFirstPushWindow: 3000
  }
};

function _default(_x, _x2, _x3) {
  return _ref.apply(this, arguments);
}

function _ref() {
  _ref = (0, _asyncToGenerator2.default)( /*#__PURE__*/_regenerator.default.mark(function _callee2(key, fetchMock, assert) {
    var segmentChangesUrlRegex, splitio, client, finish;
    return _regenerator.default.wrap(function _callee2$(_context4) {
      while (1) {
        switch (_context4.prev = _context4.next) {
          case 0:
            segmentChangesUrlRegex = new RegExp("".concat(baseUrls.sdk, "/segmentChanges/*"));
            fetchMock.getOnce(settings.url('/splitChanges?since=-1'), 500);
            fetchMock.getOnce(settings.url('/splitChanges?since=-1'), {
              status: 200,
              body: _splitchangesSince.default
            });
            fetchMock.getOnce(segmentChangesUrlRegex, {
              status: 200,
              body: {
                since: 10,
                till: 10,
                name: 'segmentName',
                added: [],
                removed: []
              }
            });
            fetchMock.getOnce(segmentChangesUrlRegex, 401);
            fetchMock.getOnce(segmentChangesUrlRegex, 500);
            fetchMock.get(segmentChangesUrlRegex, {
              status: 200,
              body: {
                since: 10,
                till: 10,
                name: 'segmentName' + (0, _now.default)(),
                added: [],
                removed: []
              }
            }); // Should not execute but adding just in case.

            fetchMock.get(settings.url('/splitChanges?since=1457552620999'), {
              status: 200,
              body: _splitchangesSince2.default
            });
            fetchMock.postOnce(settings.url('/testImpressions/bulk'), 200);
            splitio = (0, _.SplitFactory)(config);
            client = splitio.client();
            finish = /*#__PURE__*/_regenerator.default.mark(function _callee() {
              return _regenerator.default.wrap(function _callee$(_context) {
                while (1) {
                  switch (_context.prev = _context.next) {
                    case 0:
                      _context.next = 2;
                      return;

                    case 2:
                      client.destroy();
                      assert.end();

                    case 4:
                    case "end":
                      return _context.stop();
                  }
                }
              }, _callee);
            })();
            fetchMock.postOnce(settings.url('/metrics/times'), function (url, opts) {
              var data = JSON.parse(opts.body);
              assert.equal(data.length, 7, 'We performed 4 correct evaluation requests (one per method) plus ready, splits and segments, so we should have 7 latency metrics.');
              var latencyMetricsRecorded = (0, _filter.default)(data).call(data, function (metric) {
                var _context2;

                // At least one latency registed per metric
                return (0, _some.default)(_context2 = metric.latencies).call(_context2, function (count) {
                  return count > 0;
                });
              }); // If we have 7 items, it is because each one had at least ONE entry.

              assert.equal(latencyMetricsRecorded.length, 7, 'Each metric has at least one enty, matching the calls.');

              var getLatencyCount = function getLatencyCount(metricName) {
                var _context3;

                var latencyMetric = (0, _find.default)(data).call(data, function (metric) {
                  return metric.name === metricName;
                });
                if (!latencyMetric && !(0, _isArray.default)(latencyMetric.latencies)) return 0;
                return (0, _reduce.default)(_context3 = latencyMetric.latencies).call(_context3, function (accum, entry) {
                  return accum + entry;
                }, 0);
              }; // Validate both names and values.


              assert.equal(getLatencyCount('splitChangeFetcher.time'), 2, 'Two latency metrics for splitChanges');
              assert.equal(getLatencyCount('segmentChangeFetcher.time'), 1, 'One latency metric for segmentChangeFetcher');
              assert.equal(getLatencyCount('sdk.ready'), 1, 'One latency metric for ready');
              assert.equal(getLatencyCount('sdk.getTreatment'), 1, 'One latency metric for getTreatment');
              assert.equal(getLatencyCount('sdk.getTreatments'), 1, 'One latency metric for getTreatments');
              assert.equal(getLatencyCount('sdk.getTreatmentWithConfig'), 1, 'One latency metric for getTreatmentWithConfig');
              assert.equal(getLatencyCount('sdk.getTreatmentsWithConfig'), 1, 'One latency metric for getTreatmentsWithConfig');
              finish.next();
              return 200;
            });
            fetchMock.postOnce(settings.url('/metrics/counters'), function (url, opts) {
              var data = JSON.parse(opts.body);
              assert.equal(data.length, 4, 'Based on the mock setup, we should have four items.');
              var countMetricsRecorded = (0, _reduce.default)(data).call(data, function (accum, metric) {
                return accum + metric.delta;
              }, 0);

              var getRecordsCount = function getRecordsCount(metricName) {
                var countMetric = (0, _find.default)(data).call(data, function (metric) {
                  return metric.name === metricName;
                });
                if (!countMetric) return 0;
                return countMetric.delta;
              }; // 5 items:
              // For splitChanges, 1 exception and 1 200.
              // For segmentChanges (3 segments) 1 with 200, two errors.


              assert.equal(countMetricsRecorded, 5, 'Each metric has one entry, same as the amount of calls.'); // break down

              assert.equal(getRecordsCount('splitChangeFetcher.exception'), 1, 'The metric names and delta should correspond to the SDK behaviour.');
              assert.equal(getRecordsCount('segmentChangeFetcher.exception'), 2, 'The metric names and delta should correspond to the SDK behaviour.');
              assert.equal(getRecordsCount('splitChangeFetcher.status.200'), 1, 'The metric names and delta should correspond to the SDK behaviour.');
              assert.equal(getRecordsCount('segmentChangeFetcher.status.200'), 1, 'The metric names and delta should correspond to the SDK behaviour.');
              finish.next();
              return 200;
            });
            _context4.next = 16;
            return client.ready();

          case 16:
            // treatments and results are only validated so we know for sure when the function was actually running to compare the metrics.
            assert.equal(client.getTreatment(key, 'always_on'), 'on', 'Evaluation was correct.');
            assert.equal(client.getTreatment(false, 'always_on'), 'control', 'We should return control with invalid input.');
            assert.deepEqual(client.getTreatmentWithConfig(key, 'split_with_config'), {
              treatment: 'on',
              config: '{"color":"brown","dimensions":{"height":12,"width":14},"text":{"inner":"click me"}}'
            }, 'Evaluation with config was correct.');
            assert.deepEqual(client.getTreatmentWithConfig(null, 'split_with_config'), {
              treatment: 'control',
              config: null
            }, 'Evaluation with config returned control state for invalid input.');
            assert.deepEqual(client.getTreatments(key, ['always_on', 'always_off']), {
              always_on: 'on',
              always_off: 'off'
            }, 'Evaluations were correct.');
            assert.deepEqual(client.getTreatments(false, ['always_on', 'always_off', null]), {
              always_on: 'control',
              always_off: 'control'
            }, 'We should return map of controls with invalid input.');
            assert.deepEqual(client.getTreatmentsWithConfig(key, ['split_with_config', 'always_on', null]), {
              split_with_config: {
                treatment: 'on',
                config: '{"color":"brown","dimensions":{"height":12,"width":14},"text":{"inner":"click me"}}'
              },
              always_on: {
                treatment: 'on',
                config: null
              }
            }, 'Evaluations with config were correct.');
            assert.deepEqual(client.getTreatmentsWithConfig(null, ['split_with_config', 'always_on', null]), {
              split_with_config: {
                treatment: 'control',
                config: null
              },
              always_on: {
                treatment: 'control',
                config: null
              }
            }, 'Evaluations with config returned control states for invalid input.');

          case 24:
          case "end":
            return _context4.stop();
        }
      }
    }, _callee2);
  }));
  return _ref.apply(this, arguments);
}